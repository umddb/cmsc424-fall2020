{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UMD_Hackathon21_QASystem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrecW/GhV3GN4Iv8RTm48D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umddb/cmsc424-fall2020/blob/master/UMD_Hackathon21_QASystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcKrFT0JLMky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce384490-86e4-40c9-f025-4a5915d78a52"
      },
      "source": [
        "#BLOCK 1\n",
        "#Creates a folder for our Project in Google Colab\n",
        "!mkdir umd_hackathon2021\n",
        "%cd umd_hackathon2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/umd_hackathon2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z4lgn8rMXvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ce1d37-7063-4dbb-a22c-d986f295c382"
      },
      "source": [
        "#BLOCK 2 \n",
        "#Makefile script downloads training, testing and development datasets from servers of Amazon Web Services\n",
        "%%writefile /content/umd_hackathon2021/Makefile\n",
        "qanta.train.json qanta.test.json qanta.dev.json:\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.train.2018.04.18.json\n",
        "\tmv qanta.train.2018.04.18.json qanta.train.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.dev.2018.04.18.json\n",
        "\tmv qanta.dev.2018.04.18.json qanta.dev.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.test.2018.04.18.json\n",
        "\tmv qanta.test.2018.04.18.json qanta.test.json\n",
        "qanta.train.evidence.json qanta.dev.evidence.json qanta.test.evidence.json:\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_train.json\n",
        "\tmv evidence_docs_train.json qanta.train.evidence.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_dev.json\n",
        "\tmv evidence_docs_dev.json qanta.dev.evidence.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_test.json\n",
        "\tmv evidence_docs_test.json qanta.test.evidence.json\n",
        "qanta.train.evidence.text.json qanta.dev.evidence.text.json qanta.test.evidence.sent.text.json:\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_train_with_sent_text.json\n",
        "\tmv evidence_docs_train_with_sent_text.json qanta.train.evidence.text.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_dev_with_sent_text.json\n",
        "\tmv evidence_docs_dev_with_sent_text.json qanta.dev.evidence.text.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_test_with_sent_text.json\n",
        "\tmv evidence_docs_test_with_sent_text.json qanta.test.evidence.text.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/umd_hackathon2021/Makefile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A8CATHyMXkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5402d135-0b2a-4bce-d0bf-2c44affdc80a"
      },
      "source": [
        "#BLOCK 3\n",
        "#Downloads the relevant Datasets\n",
        "!make qanta.train.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.train.2018.04.18.json\n",
            "--2021-04-10 21:05:14--  https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.train.2018.04.18.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.205.120\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.205.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 142113194 (136M) [application/json]\n",
            "Saving to: ‘qanta.train.2018.04.18.json’\n",
            "\n",
            "qanta.train.2018.04 100%[===================>] 135.53M  36.0MB/s    in 4.3s    \n",
            "\n",
            "2021-04-10 21:05:19 (31.6 MB/s) - ‘qanta.train.2018.04.18.json’ saved [142113194/142113194]\n",
            "\n",
            "mv qanta.train.2018.04.18.json qanta.train.json\n",
            "wget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.dev.2018.04.18.json\n",
            "--2021-04-10 21:05:19--  https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.dev.2018.04.18.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.136.8\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3090653 (2.9M) [application/json]\n",
            "Saving to: ‘qanta.dev.2018.04.18.json’\n",
            "\n",
            "qanta.dev.2018.04.1 100%[===================>]   2.95M  5.27MB/s    in 0.6s    \n",
            "\n",
            "2021-04-10 21:05:20 (5.27 MB/s) - ‘qanta.dev.2018.04.18.json’ saved [3090653/3090653]\n",
            "\n",
            "mv qanta.dev.2018.04.18.json qanta.dev.json\n",
            "wget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.test.2018.04.18.json\n",
            "--2021-04-10 21:05:20--  https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.test.2018.04.18.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.136.8\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5873986 (5.6M) [application/json]\n",
            "Saving to: ‘qanta.test.2018.04.18.json’\n",
            "\n",
            "qanta.test.2018.04. 100%[===================>]   5.60M  8.77MB/s    in 0.6s    \n",
            "\n",
            "2021-04-10 21:05:21 (8.77 MB/s) - ‘qanta.test.2018.04.18.json’ saved [5873986/5873986]\n",
            "\n",
            "mv qanta.test.2018.04.18.json qanta.test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRpLF8LmMXaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9edbf4f-874f-4b28-b497-aaff0ac349ed"
      },
      "source": [
        "#BLOCK 4\n",
        "#K-NN Classification code to classify answers from questions\n",
        "%%writefile /content/umd_hackathon2021/knn_hack.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Sequence, Dict\n",
        "\n",
        "import numpy\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class Knearest:\n",
        "    \"\"\"\n",
        "    kNN classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, y, k):\n",
        "        \"\"\"\n",
        "        Creates a kNN instance\n",
        "        :param x: Training data input\n",
        "        :param y: Training data output\n",
        "        :param k: The number of nearest points to consider in classification\n",
        "        \"\"\"\n",
        "\n",
        "        # You may modify this code, but you shouldn't need to\n",
        "\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.k = k\n",
        "\n",
        "    def majority(self, item_indices: Sequence[int]) -> str:\n",
        "        \"\"\"Given the indices of training examples, return the majority label.\n",
        "        If there's a tie, return the one that is lexicographically\n",
        "        first (as determined by python sorted function).\n",
        "        :param item_indices: The indices of the k nearest neighbors\n",
        "        (helpfully, this is what's returned by the kneighbors\n",
        "        function.\n",
        "        \"\"\"\n",
        "        assert len(item_indices) == self.k, \"Did not get k inputs\"\n",
        "\n",
        "        # Finish this function to return the most common y value for\n",
        "        # these indices\n",
        "\n",
        "        # Gutama and Chiebuka's code block\n",
        "\n",
        "        freq = Counter(self.y[index] for index in item_indices)\n",
        "        max_count = freq.most_common(1)[0][1]\n",
        "        return sorted(i for i in freq if freq[i]  == max_count)[0]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def classify(self, example: numpy.ndarray) -> str:\n",
        "        \"\"\"\n",
        "        Given an example, classify the example.\n",
        "        :param example: A representation of an example in the same\n",
        "        format as training data\n",
        "        \"\"\"\n",
        "        # Finish this function to find the k closest points, query the\n",
        "        # majority function, and return the value.\n",
        "        # Gutama and Chiebuka's code block\n",
        "\n",
        "        possible_similarities=self.x.dot(example.T).T # [1,2,3,4,4] or [[];[];[]]\n",
        "\n",
        "        possible_similarities = (-possible_similarities).toarray()\n",
        "       \n",
        "        ind = numpy.argsort(possible_similarities, axis=1)[:, 0:self.k]\n",
        "        \n",
        "        return self.majority(ind[0])\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def confusion_matrix(self, test_x: Sequence[str], test_y: Sequence[str]) -> Dict[str, Dict[str, int]]:\n",
        "        \"\"\"\n",
        "        Given a matrix of test examples and labels, compute the confusion\n",
        "        matrixfor the current classifier.  Should return a dictionary of\n",
        "        dictionaries where d[ii][jj] is the number of times an example\n",
        "        with true label ii was labeled as jj.\n",
        "        :param test_x: Test data representation\n",
        "        :param test_y: Test data answers\n",
        "        \"\"\"\n",
        "\n",
        "        # Finish this function to build a dictionary with the\n",
        "        # mislabeled examples.  You'll need to call the classify\n",
        "        # function for each example.\n",
        "\n",
        "        d = defaultdict(dict)\n",
        "\n",
        "        # David and Nimay's code block \n",
        "\n",
        "        index = 0\n",
        "        \n",
        "        for tx, ty in zip(test_x, test_y):\n",
        "          output_label = self.classify(tx)\n",
        "          d[ty][output_label] = d[ty].get(output_label, 0) + 1\n",
        "          index += 1\n",
        "          if (index % 100 == 0):\n",
        "            print(\"%i/%i Confusion Matrix\" % (index, test_x.shape[0]))\n",
        "         \n",
        "        return d\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(confusion_matrix: Dict[str, Dict[str, int]]) -> float:\n",
        "        \"\"\"Given a confusion matrix, compute the accuracy of the underlying\n",
        "        classifier.\n",
        "        \"\"\"\n",
        "\n",
        "        # Hint: this should give you clues as to how the confusion\n",
        "        # matrix should be structured.\n",
        "\n",
        "        total = 0 \n",
        "        correct = 0\n",
        "        for ii in confusion_matrix:\n",
        "            total += sum(confusion_matrix[ii].values())\n",
        "            correct += confusion_matrix[ii].get(ii, 0)\n",
        "\n",
        "        return float(correct) / float(total)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='KNN classifier options')\n",
        "    parser.add_argument(\"--root_dir\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='../',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--train_dataset\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.train.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--test_dataset\", help=\"QB Dataset for test\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.dev.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--min_df\", help=\"How many documents must a word appear in to be feature\",\n",
        "                        type=int, default=2)\n",
        "    parser.add_argument(\"--max_df\", help=\"How many docs can words appear in and still be feature\",\n",
        "                        type=float, default=0.9)\n",
        "    parser.add_argument(\"--limit\", help=\"Number of training documents\",\n",
        "                        type=int, default=-1, required=False)\n",
        "    parser.add_argument(\"--max_ngram\", help=\"Max ngram length\", type=int, default=3)\n",
        "    parser.add_argument('--k', type=int, default=3,\n",
        "                        help=\"Number of nearest points to use\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # You should not have to modify any of this code\n",
        "    with open(os.path.join(args.root_dir, args.train_dataset)) as infile:\n",
        "        data = json.load(infile)[\"questions\"]\n",
        "        if args.limit > 0:\n",
        "            data = data[:args.limit]\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, args.max_ngram), min_df=args.min_df, max_df=args.max_df).fit(x[\"text\"] for x in data)\n",
        "    train_x = vectorizer.transform(x[\"text\"] for x in data)\n",
        "    train_y = list(x[\"page\"] for x in data)\n",
        "\n",
        "    print(type(train_x))\n",
        "\n",
        "    knn = Knearest(train_x, train_y, args.k)\n",
        "    print(\"Done loading data\")\n",
        "\n",
        "    with open(os.path.join(args.root_dir, args.test_dataset)) as infile:\n",
        "        test = json.load(infile)[\"questions\"][:100]\n",
        "\n",
        "    test_x = vectorizer.transform(x[\"text\"] for x in test)\n",
        "    test_y = list(x[\"page\"] for x in test)\n",
        "    answers = [x[0] for x in Counter(test_y).most_common(5)]\n",
        "\n",
        "    confusion = knn.confusion_matrix(test_x, test_y)\n",
        "    guesses = set()\n",
        "    for ii in answers:\n",
        "        for jj in confusion[ii]:\n",
        "            guesses.add(jj)\n",
        "\n",
        "    print(\"\\t\" + \"\\t\".join(str(x) for x in answers))\n",
        "    print(\"\".join([\"-\"] * 90))\n",
        "    for ii in guesses:\n",
        "        print(\"%30s:\\t\" % ii + \"\\t\".join(str(confusion[x].get(ii, 0))\n",
        "                                       for x in answers))\n",
        "    print(\"Accuracy: %f\" % knn.accuracy(confusion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/umd_hackathon2021/knn_hack.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIFu_QhWMXP_"
      },
      "source": [
        "#BLOCK 5\n",
        "#Test Code to see if the K-NN Classification code works as expected\n",
        "#Note: Test codes are always useful for debugging purposes\n",
        "%%writefile /content/umd_hackathon2021/test_hack.py\n",
        "\n",
        "import unittest\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "from knn_hack import *\n",
        "\n",
        "class TestKnn(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.x = array([[2, 0], [4, 1], [6, 0], [1, 4], [2, 4], [2, 5], [4, 4],\n",
        "                        [0, 2], [3, 2], [4, 2], [5, 2], [7, 3], [5, 5]])\n",
        "        self.y = array([+1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1, -1])\n",
        "        self.knn = {}\n",
        "        for ii in [1, 2, 3]:\n",
        "            self.knn[ii] = Knearest(self.x, self.y, ii)\n",
        "\n",
        "        self.queries = [array(x).reshape(1, -1) for x in\n",
        "                        [[1, 5], [0, 3], [6, 1], [6, 4]]]\n",
        "        self.test_y = [1, -1, 1, -1]\n",
        "\n",
        "    def test1(self):\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[0]), 1)\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[1]), -1)\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[2]), 1)\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[3]), -1)\n",
        "        self.assertEqual(self.knn[1].accuracy(self.knn[1].confusion_matrix(self.queries, self.test_y)), 1.0)\n",
        "\n",
        "    def test2(self):\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[0]), 1)\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[1]), -1)\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[2]), 1)\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[3]), -1)\n",
        "        self.assertEqual(self.knn[2].accuracy(self.knn[2].confusion_matrix(self.queries, self.test_y)), 1.0)\n",
        "\n",
        "\n",
        "    def test3(self):\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[0]), 1)\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[1]), 1)\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[2]), 1)\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[3]), -1)\n",
        "        self.assertEqual(self.knn[3].accuracy(self.knn[3].confusion_matrix(self.queries, self.test_y)), 0.75)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Aq1nrTMXFp"
      },
      "source": [
        "#BLOCK 6\n",
        "#Executes the test code\n",
        "!python test_hack.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBzvNhQbMWZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144d888f-0837-4e25-bc32-32e26a0135c5"
      },
      "source": [
        "#BLOCK 7\n",
        "#Executes the K-NN Classification algorithm\n",
        "!python knn_hack.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "Done loading data\n",
            "100/100 Confusion Matrix\n",
            "\tTexas_annexation\tMark_Antony\tMartin_Scorsese\tSpin_(physics)\tOperation_Condor\n",
            "------------------------------------------------------------------------------------------\n",
            "              Texas_annexation:\t1\t0\t0\t0\t0\n",
            "                Spin_(physics):\t0\t0\t0\t1\t0\n",
            "                   Mark_Antony:\t0\t1\t0\t0\t0\n",
            "                     Dirty_War:\t0\t0\t0\t0\t1\n",
            "               Martin_Scorsese:\t0\t0\t1\t0\t0\n",
            "Accuracy: 0.480000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mELGuRBGmlwO",
        "outputId": "0ba9f63b-c272-472e-91ac-d6e70dbbcc53"
      },
      "source": [
        "#Block 8 FINAL CODE (Buzz for answers above a threshold value and generate a csv file adapting knn_hack.py in Block 4)\n",
        "#K-NN Classification code to classify answers from questions\n",
        "#Please note that the automatically generated csv files had to be minimally cleaned for the purpose of the 24 hour Hackathon\n",
        "#Further research is needed to identify a good threshold value\n",
        "%%writefile /content/umd_hackathon2021/knn_hack_thresh_buzzer.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Sequence, Dict\n",
        "\n",
        "import numpy\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import csv\n",
        "\n",
        "THRESHOLD = 0.21\n",
        "\n",
        "class Knearest:\n",
        "    \"\"\"\n",
        "    kNN classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, y, k):\n",
        "        \"\"\"\n",
        "        Creates a kNN instance\n",
        "        :param x: Training data input\n",
        "        :param y: Training data output\n",
        "        :param k: The number of nearest points to consider in classification\n",
        "        \"\"\"\n",
        "\n",
        "        # You may modify this code, but you shouldn't need to\n",
        "\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.k = k\n",
        "\n",
        "    def majority(self, item_indices: Sequence[int]) -> str:\n",
        "        \"\"\"Given the indices of training examples, return the majority label.\n",
        "        If there's a tie, return the one that is lexicographically\n",
        "        first (as determined by python sorted function).\n",
        "        :param item_indices: The indices of the k nearest neighbors\n",
        "        (helpfully, this is what's returned by the kneighbors\n",
        "        function.\n",
        "        \"\"\"\n",
        "        assert len(item_indices) == self.k, \"Did not get k inputs\"\n",
        "\n",
        "        # Finish this function to return the most common y value for\n",
        "        # these indices\n",
        "\n",
        "        # Gutama and Chiebuka's code block\n",
        "        \n",
        "        freq = Counter(self.y[index] for index in item_indices)\n",
        "        max_count = freq.most_common(1)[0][1]\n",
        "        output_label = sorted(i for i in freq if freq[i]  == max_count)[0]\n",
        "        # Hardcoding for now since k = 3\n",
        "        if self.y[item_indices[0]] == output_label:\n",
        "           index_output_label = item_indices[0]\n",
        "        elif self.y[item_indices[1]] == output_label:\n",
        "           index_output_label = item_indices[1]\n",
        "        elif self.y[item_indices[2]] == output_label:\n",
        "           index_output_label = item_indices[2]\n",
        "        return output_label, index_output_label\n",
        "        return None\n",
        "\n",
        "    def classify(self, example: numpy.ndarray) -> str:\n",
        "        \"\"\"\n",
        "        Given an example, classify the example.\n",
        "        :param example: A representation of an example in the same\n",
        "        format as training data\n",
        "        \"\"\"\n",
        "        # Finish this function to find the k closest points, query the\n",
        "        # majority function, and return the value.\n",
        "        # Gutama and Chiebuka's code block\n",
        "        possible_similarities=self.x.dot(example.T).T # [1,2,3,4,4] or [[];[];[]]\n",
        "\n",
        "        possible_similarities = (-possible_similarities).toarray()\n",
        "       \n",
        "        ind = numpy.argsort(possible_similarities, axis=1)[:, 0:self.k]\n",
        "        \n",
        "        output_label, index_output_label = self.majority(ind[0])\n",
        "\n",
        "        confidence_value = self.x[index_output_label].dot(example.T).T\n",
        "        return output_label, confidence_value\n",
        "\n",
        "    def confusion_matrix(self, test_x, qid_sentid_test_x, test_y: Sequence[str]) -> Dict[str, Dict[str, int]]:\n",
        "        \"\"\"\n",
        "        Given a matrix of test examples and labels, compute the confusion\n",
        "        matrixfor the current classifier.  Should return a dictionary of\n",
        "        dictionaries where d[ii][jj] is the number of times an example\n",
        "        with true label ii was labeled as jj.\n",
        "        :param test_x: Test data representation\n",
        "        :param test_y: Test data answers\n",
        "        \"\"\"\n",
        "\n",
        "        # Finish this function to build a dictionary with the\n",
        "        # mislabeled examples.  You'll need to call the classify\n",
        "        # function for each example.\n",
        "\n",
        "        d = defaultdict(dict)\n",
        "\n",
        "        # David and Nimay's initial code block Updated by Gutama and Chiebuka for final buzzer\n",
        "\n",
        "        index = 0\n",
        "        # values to be stored in the buzz csv file\n",
        "        buzz_row_headers = ['question',\t'sentence', 'word',\t'page',\t'evidence', 'final',\t'weight'] \n",
        "        # values to be stored in the final csv file\n",
        "        final_row_headers = ['question','answer']\n",
        "        \n",
        "        buzz_file = open('UMD_Hackathon.buzz.csv', 'w', newline='')          \n",
        "        buzzwriter = csv.writer(buzz_file)\n",
        "        buzzwriter.writerow(buzz_row_headers)\n",
        "\n",
        "        final_file = open('UMD_Hackathon.final.csv', 'w', newline='')\n",
        "        finalwriter = csv.writer(final_file)\n",
        "        finalwriter.writerow(final_row_headers)\n",
        "\n",
        "        prev_qid = -1\n",
        "        final_found_flag = 0\n",
        "        past_confidence_score = 0\n",
        "        past_output_label = ''\n",
        "        past_id_buzz0 = 0\n",
        "        past_output_label_buzz0 = ''\n",
        "        final_buzz = 1\n",
        "        buzz_rows = []\n",
        "        final_rows = []\n",
        "        len_all_sentences_all_questions = len(qid_sentid_test_x)\n",
        "        \n",
        "        i = 0 \n",
        "        print(\"Data type of dict after call \",type(test_x))\n",
        "        for tx, ty in zip(test_x, test_y):\n",
        "          if (prev_qid == -1):\n",
        "              if (i<len_all_sentences_all_questions):\n",
        "                  qid = qid_sentid_test_x[i][0]\n",
        "                  sentid = qid_sentid_test_x[i][1]\n",
        "              i+=1 \n",
        "          if (prev_qid != qid):\n",
        "              prev_qid = qid\n",
        "              final_found_flag = 0\n",
        "              if final_buzz == 0:\n",
        "                 final_rows.append([past_id_buzz0, past_output_label_buzz0])\n",
        "\n",
        "\n",
        "          if (final_found_flag == 0):\n",
        "              output_label, confidence_score  = self.classify(tx)\n",
        "              \n",
        "              if (prev_qid != -1 and i<len_all_sentences_all_questions):\n",
        "                  qid = qid_sentid_test_x[i][0]\n",
        "                  sentid = qid_sentid_test_x[i][1]\n",
        "              i+=1\n",
        "              d[ty][output_label] = d[ty].get(output_label, 0) + 1\n",
        "              index += 1\n",
        "              if (index % 100 == 0):\n",
        "                  print(\"%i/%i Confusion Matrix\" % (index, test_x.shape[0]))\n",
        "          else:\n",
        "              if (i<len_all_sentences_all_questions):\n",
        "                  qid = qid_sentid_test_x[i][0]\n",
        "                  sentid = qid_sentid_test_x[i][1]\n",
        "              i+=1\n",
        "              buzz_rows.append([qid, sentid,0,past_output_label,'',1,past_confidence_score[0][0]])\n",
        "              d[ty][past_output_label] = d[ty].get(past_output_label, 0) + 1\n",
        "              index += 1\n",
        "              if (index % 100 == 0):\n",
        "                  print(\"%i/%i Confusion Matrix\" % (index, test_x.shape[0]))\n",
        "              continue\n",
        "\n",
        "                                                        \n",
        "          if (confidence_score[0][0]>=THRESHOLD):\n",
        "                  final_buzz = 1   \n",
        "                  buzz_rows.append([qid, sentid,0,output_label,'',final_buzz,confidence_score[0][0]])\n",
        "\n",
        "                  if (final_found_flag != 1):\n",
        "                    final_rows.append([qid, output_label])\n",
        "                  final_found_flag = 1\n",
        "                  past_confidence_score = confidence_score[0][0]\n",
        "                  past_output_label = output_label\n",
        "          else:\n",
        "                  final_buzz = 0 #TASK 5 Value goes to csv file along with associate confidence_score and sentence_number (which is the iteration number in this current for loop)\n",
        "                  buzz_rows.append([qid, sentid,0,output_label,'',final_buzz,confidence_score[0][0]])\n",
        "                  \n",
        "                  past_id_buzz0 = qid\n",
        "                  past_output_label_buzz0 = output_label\n",
        "        \n",
        "\n",
        "        buzzwriter.writerows(buzz_rows)\n",
        "        finalwriter.writerows(final_rows)\n",
        "\n",
        "        return d\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(confusion_matrix: Dict[str, Dict[str, int]]) -> float:\n",
        "        \"\"\"Given a confusion matrix, compute the accuracy of the underlying\n",
        "        classifier.\n",
        "        \"\"\"\n",
        "\n",
        "        # Hint: this should give you clues as to how the confusion\n",
        "        # matrix should be structured.\n",
        "\n",
        "        total = 0 \n",
        "        correct = 0\n",
        "        for ii in confusion_matrix:\n",
        "            total += sum(confusion_matrix[ii].values())\n",
        "            correct += confusion_matrix[ii].get(ii, 0)\n",
        "        \n",
        "        return float(correct) / float(total)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='KNN classifier options')\n",
        "    parser.add_argument(\"--root_dir\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='../',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--train_dataset\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.train.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--test_dataset\", help=\"QB Dataset for test\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.dev.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--min_df\", help=\"How many documents must a word appear in to be feature\",\n",
        "                        type=int, default=2)\n",
        "    parser.add_argument(\"--max_df\", help=\"How many docs can words appear in and still be feature\",\n",
        "                        type=float, default=0.9)\n",
        "    parser.add_argument(\"--limit\", help=\"Number of training documents\",\n",
        "                        type=int, default=-1, required=False)\n",
        "    parser.add_argument(\"--max_ngram\", help=\"Max ngram length\", type=int, default=3)\n",
        "    parser.add_argument('--k', type=int, default=3,\n",
        "                        help=\"Number of nearest points to use\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # You should not have to modify any of this code\n",
        "    with open(os.path.join(args.root_dir, args.train_dataset)) as infile:\n",
        "        data = json.load(infile)[\"questions\"]\n",
        "        if args.limit > 0:\n",
        "            data = data[:args.limit]\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, args.max_ngram), min_df=args.min_df, max_df=args.max_df).fit(x[\"text\"] for x in data)\n",
        "    train_x = vectorizer.transform(x[\"text\"] for x in data)\n",
        "    train_y = list(x[\"page\"] for x in data)\n",
        "\n",
        "    knn = Knearest(train_x, train_y, args.k)\n",
        "    print(\"Done loading data\")\n",
        "\n",
        "    dict_test_x = {} \n",
        "    test_y = []\n",
        "    all_questions = []\n",
        "    with open('UMD_Hackathon.csv', 'r', newline='') as csvfile: \n",
        "         reader = csv.DictReader(csvfile)\n",
        "         prev_qid = -1 \n",
        "         prev_qid_list = []\n",
        "         all_testx_vec = []\n",
        "         for row in reader: \n",
        "             if (prev_qid!=row[\"id\"] and prev_qid!=-1):\n",
        "               prev_qid = row[\"id\"]\n",
        "               vec_test_x = vectorizer.transform(each_sent_question for each_sent_question in per_question) \n",
        "               all_testx_vec.append(vec_test_x)\n",
        "               per_question = []\n",
        "               test_y.append(row[\"answer\"])\n",
        "\n",
        "             if prev_qid == -1:\n",
        "               test_y.append(row[\"answer\"])\n",
        "               prev_qid = row[\"id\"]\n",
        "               #per_question = ''\n",
        "               per_question = []\n",
        "\n",
        "             if (prev_qid == row[\"id\"]):\n",
        "               prev_qid_list.append([row[\"id\"],row[\"sent\"]])\n",
        "               per_question.append(row[\"text\"])\n",
        "    \n",
        "    #For the last question\n",
        "    vec_test_x = vectorizer.transform(each_sent_question for each_sent_question in per_question) \n",
        "    all_testx_vec.append(vec_test_x)\n",
        "\n",
        "    confusion = knn.confusion_matrix(all_testx_vec, prev_qid_list, test_y)\n",
        "\n",
        "    #print(\"Data type of dict b4 call \",type(dict_test_x))\n",
        "    #confusion = knn.confusion_matrix(dict_test_x, test_y)\n",
        "    \n",
        "    answers = [x[0] for x in Counter(test_y).most_common(5)]\n",
        "\n",
        "    guesses = set()\n",
        "    for ii in answers:\n",
        "        for jj in confusion[ii]:\n",
        "            guesses.add(jj)\n",
        "\n",
        "    print(\"\\t\" + \"\\t\".join(str(x) for x in answers))\n",
        "    print(\"\".join([\"-\"] * 90))\n",
        "    for ii in guesses:\n",
        "        print(\"%30s:\\t\" % ii + \"\\t\".join(str(confusion[x].get(ii, 0))\n",
        "                                       for x in answers))\n",
        "    print(\"Accuracy: %f\" % knn.accuracy(confusion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/umd_hackathon2021/knn_hack_thresh_buzzer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC1Wr1cBmsqS"
      },
      "source": [
        "!python3 knn_hack_thresh_buzzer.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}